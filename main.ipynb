{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This repo demonstrates how using the HuggingFaceInferenceAPI LLM when building a Property Graph with a SchemaLLMPathExtractor fails to extract entities where the same settings succeed when using Ollama.\n",
    "\n",
    "Most of this code is taken directly from the example here, https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_advanced.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-graph-stores-neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema and extractor\n",
    "\n",
    "from typing import Literal\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "\n",
    "# best practice to use upper-case\n",
    "entities = Literal[\"PERSON\", \"PLACE\", \"ORGANIZATION\"]\n",
    "relations = Literal[\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"]\n",
    "\n",
    "# define which entities can have which relations\n",
    "validation_schema = {\n",
    "    \"PERSON\": [\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"],\n",
    "    \"PLACE\": [\"HAS\", \"PART_OF\", \"WORKED_AT\"],\n",
    "    \"ORGANIZATION\": [\"HAS\", \"PART_OF\", \"WORKED_WITH\"],\n",
    "}\n",
    "validation_schema = [\n",
    "    (\"ORGANIZATION\", \"HAS\", \"PERSON\"),\n",
    "    (\"PERSON\", \"WORKED_AT\", \"ORGANIZATION\"),\n",
    "    (\"PERSON\", \"WORKED_WITH\", \"PERSON\"),\n",
    "    (\"PERSON\", \"WORKED_ON\", \"ORGANIZATION\"),\n",
    "    (\"PERSON\", \"PART_OF\", \"ORGANIZATION\"),\n",
    "    (\"ORGANIZATION\", \"PART_OF\", \"ORGANIZATION\"),\n",
    "    (\"PERSON\", \"WORKED_AT\", \"PLACE\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setup Neo 4j\n",
    "I am running this locally using Neo4j Desktop on a Windows computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "\n",
    "password = os.getenv(\"NEO_4J_PASSWORD\")\n",
    "\n",
    "graph_store = Neo4jPropertyGraphStore(\n",
    "    username=\"neo4j\",\n",
    "    password=password,\n",
    "    url=\"bolt://localhost:7687\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=Ollama(model=\"llama3\", json_mode=True, request_timeout=3600),\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    kg_extractors=[kg_extractor],\n",
    "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Results\n",
    "This is a picture from Neo4j desktop using the query, MATCH(n:\\`\\_\\_Entity\\_\\_\\`) RETURN n\n",
    "\n",
    "![alt text](pictures/ollama_schema.svg \"Hugging Face Simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index with Huggingface Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "token = os.getenv(\"HUGGING_FACE_KEY\")\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm=HuggingFaceInferenceAPI(model_name=\"meta-llama/Meta-Llama-3-70B-Instruct\", token=token),\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    kg_extractors=[kg_extractor],\n",
    "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Inference API Results\n",
    "This is a picture from Neo4j desktop using the query, MATCH(n) RETURN n\n",
    "\n",
    "You can see that no entities are extracted, only chunks.\n",
    "\n",
    "\n",
    "![alt text](pictures/hf_schema.svg \"Hugging Face Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Index with Hugging Face Inference API and SimpleLLMPathExtractor\n",
    "As a bonus, here is a demo of the inference api working with the simple llm path extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "\n",
    "token = os.getenv(\"HUGGING_FACE_KEY\")\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "kg_extractor = SimpleLLMPathExtractor(\n",
    "    llm=HuggingFaceInferenceAPI(model_name=\"meta-llama/Meta-Llama-3-70B-Instruct\", token=token),\n",
    "    max_paths_per_chunk=10,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    kg_extractors=[kg_extractor],\n",
    "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Inference API Results with Simple LLM Path Extractor\n",
    "This is a picture from Neo4j desktop using the query, MATCH(n:`__Entity__`) RETURN n\n",
    "\n",
    "\n",
    "![alt text](pictures/hf_simple.svg \"Hugging Face Simple\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
